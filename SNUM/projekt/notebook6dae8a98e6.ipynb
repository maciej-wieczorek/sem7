{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction \n","Autoencoder are special type of deep learning architecture that consist of two networks encoder and decoder.\n","The encoder, through a series of CNN and downsampling, learns a reduced dimensional representation of the input data while decoder  through the use of CNN and upsampling, attempts to regenerate the data from the these representations. A well-trained decoder is able to regenerated data that is identical or as close as possible to the original input data.\n","Autoencoder are generally used for anamoly detection, denoising image, colorizing the images. Here, i am going to colorize the landscape images using autoencoder."]},{"cell_type":"markdown","metadata":{},"source":["<img src = 'https://miro.medium.com/max/600/1*nqzWupxC60iAH2dYrFT78Q.png' >"]},{"cell_type":"markdown","metadata":{},"source":["## Image Colorization\n","Image colorization using different softwares require large amount of human effort, time and skill.But special type of deep learning architecture called autoencoder has made this task quiet easy. Automatic image colorization often involves the use of a class of convolutional neural networks (CNN) called autoencoders. These neural networks are able to distill the salient features of an image, and then regenerate the image based on these learned features. "]},{"cell_type":"markdown","metadata":{},"source":["<img src = \"https://tinyclouds.org/colorize/best/6.jpg\">"]},{"cell_type":"markdown","metadata":{},"source":["## Import necessary libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import keras\n","import cv2\n","from tensorflow.keras.layers import MaxPool2D,Conv2D,UpSampling2D,Input,Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.image import img_to_array\n","import os\n","from tqdm import tqdm\n","import re\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### Getting landscape image data,resizing them and appending in array\n","To get the image in sorted order i have defined the function sorted_alphanumeric. Here, I have used open cv library to read and resize images. Finally images are normalized and are converted to array and are appended in empty list"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] The system cannot find the path specified: '../input/landscape-image-colorization/landscape Images/color'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m color_img \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/landscape-image-colorization/landscape Images/color\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m files \u001b[38;5;241m=\u001b[39m sorted_alphanumeric(files)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(files):    \n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../input/landscape-image-colorization/landscape Images/color'"]}],"source":["# to get the files in proper order\n","def sorted_alphanumeric(data):  \n","    convert = lambda text: int(text) if text.isdigit() else text.lower()\n","    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n","    return sorted(data,key = alphanum_key)\n","# defining the size of the image\n","SIZE = 160\n","color_img = []\n","path = '../input/landscape-image-colorization/landscape Images/color'\n","files = os.listdir(path)\n","files = sorted_alphanumeric(files)\n","for i in tqdm(files):    \n","    if i == '6000.jpg':\n","        break\n","    else:    \n","        img = cv2.imread(path + '/'+i,1)\n","        # open cv reads images in BGR format so we have to convert it to RGB\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        #resizing image\n","        img = cv2.resize(img, (SIZE, SIZE))\n","        img = img.astype('float32') / 255.0\n","        color_img.append(img_to_array(img))\n","\n","\n","gray_img = []\n","path = '../input/landscape-image-colorization/landscape Images/gray'\n","files = os.listdir(path)\n","files = sorted_alphanumeric(files)\n","for i in tqdm(files):\n","     if i == '6000.jpg':\n","        break\n","     else: \n","        img = cv2.imread(path + '/'+i,1)\n","\n","        #resizing image\n","        img = cv2.resize(img, (SIZE, SIZE))\n","        img = img.astype('float32') / 255.0\n","        gray_img.append(img_to_array(img))\n","         \n","   "]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Color image and it's corresponding grayscale image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# defining function to plot images pair\n","def plot_images(color,grayscale):\n","    plt.figure(figsize=(15,15))\n","    plt.subplot(1,3,1)\n","    plt.title('Color Image', color = 'green', fontsize = 20)\n","    plt.imshow(color)\n","    plt.subplot(1,3,2)\n","    plt.title('Grayscale Image ', color = 'black', fontsize = 20)\n","    plt.imshow(grayscale)\n","   \n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Plotting image pair**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(3,10):\n","     plot_images(color_img[i],gray_img[i])"]},{"cell_type":"markdown","metadata":{},"source":["### Slicing and reshaping\n","Out of 5000 images I have sliced them to two part. train images consist 4000 images  while test images contains 1000 images.\n","After slicing the image array, I reshaped them so that images can be fed directly into our encoder network"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_gray_image = gray_img[:5500]\n","train_color_image = color_img[:5500]\n","\n","test_gray_image = gray_img[5500:]\n","test_color_image = color_img[5500:]\n","# reshaping\n","train_g = np.reshape(train_gray_image,(len(train_gray_image),SIZE,SIZE,3))\n","train_c = np.reshape(train_color_image, (len(train_color_image),SIZE,SIZE,3))\n","print('Train color image shape:',train_c.shape)\n","\n","\n","test_gray_image = np.reshape(test_gray_image,(len(test_gray_image),SIZE,SIZE,3))\n","test_color_image = np.reshape(test_color_image, (len(test_color_image),SIZE,SIZE,3))\n","print('Test color image shape',test_color_image.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Defining our model\n","Encoder layer of our model consist blocks of Convolution layer with different number of kernel and kernel_size. Here, Convolution is used for downsampling.\n","Similary, Decoder layer of our model consist of  transpose convolution layer with different kernel size. Here, Decoder layer upsample image downsampled by encoder.\n","Since there is feature loss between the encoder and decoder layers so inorder to prevent feature loss i have concatenate corresponding encoder and decoder layers. Check U_Net architecture for better understanding......"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras import layers\n","def down(filters , kernel_size, apply_batch_normalization = True):\n","    downsample = tf.keras.models.Sequential()\n","    downsample.add(layers.Conv2D(filters,kernel_size,padding = 'same', strides = 2))\n","    if apply_batch_normalization:\n","        downsample.add(layers.BatchNormalization())\n","    downsample.add(keras.layers.LeakyReLU())\n","    return downsample\n","\n","\n","def up(filters, kernel_size, dropout = False):\n","    upsample = tf.keras.models.Sequential()\n","    upsample.add(layers.Conv2DTranspose(filters, kernel_size,padding = 'same', strides = 2))\n","    if dropout:\n","        upsample.dropout(0.2)\n","    upsample.add(keras.layers.LeakyReLU())\n","    return upsample\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def model():\n","    inputs = layers.Input(shape= [160,160,3])\n","    d1 = down(128,(3,3),False)(inputs)\n","    d2 = down(128,(3,3),False)(d1)\n","    d3 = down(256,(3,3),True)(d2)\n","    d4 = down(512,(3,3),True)(d3)\n","    \n","    d5 = down(512,(3,3),True)(d4)\n","    #upsampling\n","    u1 = up(512,(3,3),False)(d5)\n","    u1 = layers.concatenate([u1,d4])\n","    u2 = up(256,(3,3),False)(u1)\n","    u2 = layers.concatenate([u2,d3])\n","    u3 = up(128,(3,3),False)(u2)\n","    u3 = layers.concatenate([u3,d2])\n","    u4 = up(128,(3,3),False)(u3)\n","    u4 = layers.concatenate([u4,d1])\n","    u5 = up(3,(3,3),False)(u4)\n","    u5 = layers.concatenate([u5,inputs])\n","    output = layers.Conv2D(3,(2,2),strides = 1, padding = 'same')(u5)\n","    return tf.keras.Model(inputs=inputs, outputs=output)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = model()\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Fitting our model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mean_absolute_error',\n","              metrics = ['acc'])\n","\n","model.fit(train_g, train_c, epochs = 50,batch_size = 50,verbose = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.evaluate(test_gray_image,test_color_image)"]},{"cell_type":"markdown","metadata":{},"source":["# plotting colorized image along with grayscale and color image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# defining function to plot images pair\n","def plot_images(color,grayscale,predicted):\n","    plt.figure(figsize=(15,15))\n","    plt.subplot(1,3,1)\n","    plt.title('Color Image', color = 'green', fontsize = 20)\n","    plt.imshow(color)\n","    plt.subplot(1,3,2)\n","    plt.title('Grayscale Image ', color = 'black', fontsize = 20)\n","    plt.imshow(grayscale)\n","    plt.subplot(1,3,3)\n","    plt.title('Predicted Image ', color = 'Red', fontsize = 20)\n","    plt.imshow(predicted)\n","   \n","    plt.show()\n","\n","for i in range(50,58):\n","    predicted = np.clip(model.predict(test_gray_image[i].reshape(1,SIZE, SIZE,3)),0.0,1.0).reshape(SIZE, SIZE,3)\n","    plot_images(test_color_image[i],test_gray_image[i],predicted)\n","\n"," "]},{"cell_type":"markdown","metadata":{},"source":["# Thanks for your visit.\n","## Any suggestions to improve this model is highly appreciated.\n","# Feel free to  comment"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1036526,"sourceId":1913658,"sourceType":"datasetVersion"}],"dockerImageVersionId":30042,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"nbformat":4,"nbformat_minor":4}
