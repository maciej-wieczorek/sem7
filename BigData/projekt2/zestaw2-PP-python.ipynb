{"cells": [{"cell_type": "markdown", "id": "861b6d4a-0de6-42ba-97a5-beef1f82f292", "metadata": {}, "source": "# Projekt Apache Spark"}, {"cell_type": "markdown", "id": "7b301ae8-ceff-4dbf-8d04-75bb4eb52480", "metadata": {}, "source": "# Wprowadzenie\n\nWykorzystuj\u0105c ten notatnik jako szablon zrealizuj projekt Apache Spark zgodnie z przydzielonym zestawem. \n\nKilka uwag:\n\n* Nie modyfikuj ani nie usuwaj paragraf\u00f3w *markdown* w tym notatniku, chyba \u017ce wynika to jednoznacznie z instrukcji. \n* Istniej\u0105ce paragrafy zawieraj\u0105ce *kod* uzupe\u0142nij w razie potrzeby zgodnie z instrukcjami\n    - nie usuwaj ich\n    - nie usuwaj zawartych w nich instrukcji oraz kodu\n    - nie modyfikuj ich, je\u015bli instrukcje jawnie tego nie nakazuj\u0105\n* Mo\u017cesz dodawa\u0107 nowe paragrafy zar\u00f3wno zawieraj\u0105ce kod jak i komentarze dotycz\u0105ce tego kodu (markdown)"}, {"cell_type": "markdown", "id": "e69d12f1-1013-4c74-b6aa-686ccfcbdd5c", "metadata": {}, "source": "# Tre\u015b\u0107 projektu\n\nPoni\u017cej w paragrafie markdown wstaw tytu\u0142 przydzielonego zestawu"}, {"cell_type": "markdown", "id": "adfc4ff6-4d43-49ed-a0d1-8b6988eaec16", "metadata": {}, "source": "# Zestaw 2 \u2013 nyc-taxi\n\n**Uwaga**\n\n- W ramach wzorca nie s\u0105 spe\u0142nione \u017cadne regu\u0142y projektu. \n- Brak konsekwencji w wykorzystaniu w\u0142a\u015bciwego API w ramach poszczeg\u00f3lnych cz\u0119\u015bci\n- Zadanie *misji g\u0142\u00f3wnej* polega na zliczeniu s\u0142\u00f3wek.  "}, {"cell_type": "markdown", "id": "5e128e43-6cce-4ffa-9609-9fae4b164ae9", "metadata": {}, "source": "# Dzia\u0142ania wst\u0119pne \n\nUruchom poni\u017cszy paragraf, aby utworzy\u0107 obiekty kontekstu Sparka. Je\u015bli jest taka potrzeba dostosuj te polecenia. Pami\u0119taj po potrzebnych bibliotekach."}, {"cell_type": "code", "execution_count": 1, "id": "26fb1050-386f-4398-ba5a-b45f5065d87b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/01/10 21:06:53 INFO SparkEnv: Registering MapOutputTracker\n24/01/10 21:06:53 INFO SparkEnv: Registering BlockManagerMaster\n24/01/10 21:06:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/01/10 21:06:53 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n\n# Spark session & context\nspark = SparkSession.builder.getOrCreate()\n\nsc = spark.sparkContext"}, {"cell_type": "markdown", "id": "8695a354-52bc-4bba-8222-7121bf07ae90", "metadata": {}, "source": "W poni\u017cszym paragrafie uzupe\u0142nij polecenia definiuj\u0105ce poszczeg\u00f3lne zmienne. \n\nPami\u0119taj aby\u015b:\n\n* w p\u00f3\u017aniejszym kodzie, dla wszystkich cze\u015bci projektu, korzysta\u0142 z tych zdefiniowanych zmiennych. Wykorzystuj je analogicznie jak parametry\n* przed ostateczn\u0105 rejestracj\u0105 projektu usun\u0105\u0142 ich warto\u015bci, tak aby nie pozostawia\u0107 w notatniku niczego co mog\u0142oby identyfikowa\u0107 Ciebie jako jego autora"}, {"cell_type": "code", "execution_count": 2, "id": "e883af01-7117-4faa-a840-7ff807a195d9", "metadata": {}, "outputs": [], "source": "# pe\u0142na \u015bcie\u017cka do katalogu w zasobniku zawieraj\u0105cego podkatalogi `datasource1` i `datasource4` \n# z danymi \u017ar\u00f3d\u0142owymi\ninput_dir = \"/home/maciejmail_wieczorek/projekt1/input\""}, {"cell_type": "markdown", "id": "4601cc7a-3ed5-47e2-994f-ebec642049b5", "metadata": {}, "source": "Nie modyfikuj poni\u017cszych paragraf\u00f3w. Wykonaj je i u\u017cywaj zdefniowanych poni\u017cej zmiennych jak parametr\u00f3w Twojego programu."}, {"cell_type": "code", "execution_count": 3, "id": "6167e297-01ed-463e-bb81-9104d7cf7093", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n# \u015bcie\u017cki dla danych \u017ar\u00f3d\u0142owych \ndatasource1_dir = input_dir + \"/datasource1\"\ndatasource4_dir = input_dir + \"/datasource4\"\n\n# nazwy i \u015bcie\u017cki dla wynik\u00f3w dla misji g\u0142\u00f3wnej \n# cz\u0119\u015b\u0107 1 (Spark Core - RDD) \nrdd_result_dir = \"/tmp/output1\"\n\n# cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame)\ndf_result_table = \"output2\"\n\n# cz\u0119\u015b\u0107 3 (Pandas API on Spark)\nps_result_file = \"/tmp/output3.json\""}, {"cell_type": "code", "execution_count": 4, "id": "e36e0314-a4ac-4096-9e4b-23fd4a73e0a9", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nimport os\ndef remove_file(file):\n    if os.path.exists(file):\n        os.remove(file)\n\nremove_file(\"metric_functions.py\")\nremove_file(\"tools_functions.py\")"}, {"cell_type": "code", "execution_count": 5, "id": "1b4b8e00-10ae-47dc-b623-d1dacbe9c86b", "metadata": {}, "outputs": [{"data": {"text/plain": "3322"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "# NIE ZMIENIA\u0106\nimport requests\nr = requests.get(\"https://jankiewicz.pl/bigdata/metric_functions.py\", allow_redirects=True)\nopen('metric_functions.py', 'wb').write(r.content)\nr = requests.get(\"https://jankiewicz.pl/bigdata/tools_functions.py\", allow_redirects=True)\nopen('tools_functions.py', 'wb').write(r.content)"}, {"cell_type": "code", "execution_count": 6, "id": "0a433894-dc97-46f2-be51-9f40fa36894f", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n%run metric_functions.py\n%run tools_functions.py"}, {"cell_type": "markdown", "id": "c9d3a9dc-ac3b-4316-abb9-365caa1d7185", "metadata": {}, "source": "Poni\u017csze paragrafy maj\u0105 na celu usun\u0105\u0107 ewentualne pozosta\u0142o\u015bci poprzednich uruchomie\u0144 tego lub innych notatnik\u00f3w"}, {"cell_type": "code", "execution_count": 7, "id": "08091c72-937f-41c2-9afe-d1505862bf1c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "rm: `/tmp/output1': No such file or directory\n"}, {"name": "stdout", "output_type": "stream", "text": "Error deleting file /tmp/output1: Command '['hadoop', 'fs', '-rm', '-r', '/tmp/output1']' returned non-zero exit status 1.\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 1 (Spark Core - RDD) \ndelete_dir(spark, rdd_result_dir)"}, {"cell_type": "code", "execution_count": 8, "id": "f3e863c0-c824-47bd-b53a-ce3b1fd6d453", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "The table output2 does not exist.\nError deleting file file:/spark-warehouse/output2: Command '['hadoop', 'fs', '-rm', '-r', 'file:/spark-warehouse/output2']' returned non-zero exit status 1.\n"}, {"name": "stderr", "output_type": "stream", "text": "rm: `file:/spark-warehouse/output2': No such file or directory\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame) \ndrop_table(spark, df_result_table)"}, {"cell_type": "code", "execution_count": 9, "id": "72956a1a-da48-4d2b-a07a-e03d56431d6e", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 3 (Pandas API on Spark) \nremove_file(ps_result_file)"}, {"cell_type": "code", "execution_count": 10, "id": "b9e423d4-92b8-4161-98da-1a867f86d780", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://pbd-cluster-m.europe-central2-c.c.big-data-2023-10-mw.internal:35883\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fddc243f580>"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "# NIE ZMIENIA\u0106\nspark"}, {"cell_type": "markdown", "id": "14faf05b-6c52-4b02-b2e5-2ddb3f38c704", "metadata": {}, "source": "***Uwaga!***\n\nUruchom poni\u017cszy paragraf i sprawd\u017a czy adres, pod kt\u00f3rym dost\u0119pny *Apache Spark Application UI* jest poprawny wywo\u0142uj\u0105c nast\u0119pny testowy paragraf. \n\nW razie potrzeby okre\u015bl samodzielnie poprawny adres, pod kt\u00f3rym dost\u0119pny *Apache Spark Application UI*"}, {"cell_type": "code", "execution_count": 11, "id": "32acf3d2-ec4e-469d-bb0b-5f260c2c8e3b", "metadata": {}, "outputs": [{"data": {"text/plain": "'http://pbd-cluster-m.europe-central2-c.c.big-data-2023-10-mw.internal:35883'"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "# adres URL, pod kt\u00f3rym dost\u0119pny Apache Spark Application UI (REST API)\n# \nspark_ui_address = extract_host_and_port(spark, \"http://localhost:4041\")\nspark_ui_address"}, {"cell_type": "code", "execution_count": 12, "id": "32c2329e-1d7a-465f-a23b-333f95bf7deb", "metadata": {}, "outputs": [{"data": {"text/plain": "{'numTasks': 0,\n 'numActiveTasks': 0,\n 'numCompleteTasks': 0,\n 'numFailedTasks': 0,\n 'numKilledTasks': 0,\n 'numCompletedIndices': 0,\n 'executorDeserializeTime': 0,\n 'executorDeserializeCpuTime': 0,\n 'executorRunTime': 0,\n 'executorCpuTime': 0,\n 'resultSize': 0,\n 'jvmGcTime': 0,\n 'resultSerializationTime': 0,\n 'memoryBytesSpilled': 0,\n 'diskBytesSpilled': 0,\n 'peakExecutionMemory': 0,\n 'inputBytes': 0,\n 'inputRecords': 0,\n 'outputBytes': 0,\n 'outputRecords': 0,\n 'shuffleRemoteBlocksFetched': 0,\n 'shuffleLocalBlocksFetched': 0,\n 'shuffleFetchWaitTime': 0,\n 'shuffleRemoteBytesRead': 0,\n 'shuffleRemoteBytesReadToDisk': 0,\n 'shuffleLocalBytesRead': 0,\n 'shuffleReadBytes': 0,\n 'shuffleReadRecords': 0,\n 'shuffleWriteBytes': 0,\n 'shuffleWriteTime': 0,\n 'shuffleWriteRecords': 0}"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "# testowy paragraf\ntest_metrics = get_current_metrics(spark_ui_address)\ntest_metrics"}, {"cell_type": "markdown", "id": "f5ccca69-c577-440c-aa5c-c9df3a54e127", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 1 - Spark Core (RDD)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "f0af3440-983a-4cac-a8e7-4908b010947c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "5fc37879-e0fa-4c4a-bd0d-4c01c3ecf38a", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "d303a72b-4083-470e-b25d-3224360ee94f", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 13, "id": "037689d7-f0ee-4165-bef0-83fa7f3e8346", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "b23971c0-cec7-4ea8-befb-7f063dce863c", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** *misji g\u0142\u00f3wnej* oparte na *RDD API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *RDD API* tego wymaga. \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 71, "id": "8af00c41-02a9-4a85-b3c6-bc41098edbe2", "metadata": {}, "outputs": [], "source": "# Wczytanie plik\u00f3w tekstowych\ntrips_rdd = sc.textFile(datasource1_dir)\ntaxi_zones_rdd = sc.textFile(datasource4_dir)"}, {"cell_type": "code", "execution_count": 78, "id": "b1350a0c-6017-4517-9f93-0c476eae93d4", "metadata": {}, "outputs": [], "source": "trips_rdd = trips_rdd.map(lambda x: x.split(','))"}, {"cell_type": "code", "execution_count": 79, "id": "0a7f0598-9da3-4478-a286-4f5bedcfc470", "metadata": {}, "outputs": [{"data": {"text/plain": "[['2',\n  '2018-12-12 15:13:24',\n  '2018-12-12 15:50:10',\n  '1',\n  '5.99',\n  '1',\n  'N',\n  '231',\n  '237',\n  '1',\n  '27',\n  '0',\n  '0.5',\n  '5.56',\n  '0',\n  '0.3',\n  '33.36'],\n ['2',\n  '2018-12-06 14:03:48',\n  '2018-12-06 14:34:49',\n  '1',\n  '2.95',\n  '1',\n  'N',\n  '161',\n  '75',\n  '1',\n  '19',\n  '0',\n  '0.5',\n  '5.94',\n  '0',\n  '0.3',\n  '25.74'],\n ['2',\n  '2018-12-20 10:03:07',\n  '2018-12-20 10:12:35',\n  '2',\n  '1.23',\n  '1',\n  'N',\n  '237',\n  '161',\n  '1',\n  '7.5',\n  '0',\n  '0.5',\n  '1.66',\n  '0',\n  '0.3',\n  '9.96']]"}, "execution_count": 79, "metadata": {}, "output_type": "execute_result"}], "source": "trips_rdd.take(3)"}, {"cell_type": "code", "execution_count": 80, "id": "16f2fb80-212f-44f2-80e7-e385c98cba38", "metadata": {}, "outputs": [], "source": "taxi_zones_header = taxi_zones_rdd.first()\ntaxi_zones_rdd = taxi_zones_rdd.filter(lambda x: x != taxi_zones_header)"}, {"cell_type": "code", "execution_count": 82, "id": "4e8eed86-dc51-4510-8994-dd18e1cdf3a4", "metadata": {}, "outputs": [{"data": {"text/plain": "[['1', '\"EWR\"', '\"Newark Airport\"', '\"EWR\"'],\n ['2', '\"Queens\"', '\"Jamaica Bay\"', '\"Boro Zone\"'],\n ['3', '\"Bronx\"', '\"Allerton/Pelham Gardens\"', '\"Boro Zone\"']]"}, "execution_count": 82, "metadata": {}, "output_type": "execute_result"}], "source": "taxi_zones_rdd = taxi_zones_rdd.map(lambda x: x.split(','))\ntaxi_zones_rdd.take(3)"}, {"cell_type": "code", "execution_count": 83, "id": "fdcd98b9-55f1-45f8-bb7f-d18de88ebf16", "metadata": {}, "outputs": [], "source": "filtered_trips_rdd = trips_rdd.filter(lambda x: x[9] == '2')  # Filtrujemy tylko got\u00f3wkowe p\u0142atno\u015bci"}, {"cell_type": "code", "execution_count": 84, "id": "53ee9a75-9d02-447e-96e4-c96202952b5c", "metadata": {}, "outputs": [{"data": {"text/plain": "[['1',\n  '2018-12-01 10:50:49',\n  '2018-12-01 10:54:00',\n  '1',\n  '.50',\n  '1',\n  'N',\n  '226',\n  '226',\n  '2',\n  '4',\n  '0',\n  '0.5',\n  '0',\n  '0',\n  '0.3',\n  '4.8'],\n ['2',\n  '2018-12-19 21:03:10',\n  '2018-12-19 21:07:43',\n  '2',\n  '.65',\n  '1',\n  'N',\n  '164',\n  '68',\n  '2',\n  '5',\n  '0.5',\n  '0.5',\n  '0',\n  '0',\n  '0.3',\n  '6.3'],\n ['1',\n  '2018-12-16 17:44:12',\n  '2018-12-16 18:01:53',\n  '1',\n  '1.60',\n  '1',\n  'N',\n  '246',\n  '113',\n  '2',\n  '11.5',\n  '0',\n  '0.5',\n  '0',\n  '0',\n  '0.3',\n  '12.3']]"}, "execution_count": 84, "metadata": {}, "output_type": "execute_result"}], "source": "filtered_trips_rdd.take(3)"}, {"cell_type": "code", "execution_count": 85, "id": "7a2d2083-0499-4d9c-8270-6fad38ea884b", "metadata": {}, "outputs": [], "source": "# Mapowanie dla trips: ((month, borough), (passenger_count, total_amount, trip_distance, pickup_date))\nmapped_trips_rdd = filtered_trips_rdd.map(lambda x: ((x[1][:7], x[7]), \n                                                     (int(x[3]), float(x[16]), float(x[4]), x[1][:10])))"}, {"cell_type": "code", "execution_count": 86, "id": "e4229441-d16f-42a9-89d6-90aab96bacf5", "metadata": {}, "outputs": [{"data": {"text/plain": "[(('2018-12', '226'), (1, 4.8, 0.5, '2018-12-01')),\n (('2018-12', '164'), (2, 6.3, 0.65, '2018-12-19')),\n (('2018-12', '246'), (1, 12.3, 1.6, '2018-12-16'))]"}, "execution_count": 86, "metadata": {}, "output_type": "execute_result"}], "source": "mapped_trips_rdd.take(3)"}, {"cell_type": "code", "execution_count": 87, "id": "2072429c-092a-488c-b621-d0c8dcd0e073", "metadata": {}, "outputs": [], "source": "# Redukcja dla trips: ((month, borough), (suma_passenger_count, suma_total_amount, suma_trip_distance, max_day))\nreduced_trips_rdd = mapped_trips_rdd.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2], max(a[3], b[3])))"}, {"cell_type": "code", "execution_count": 93, "id": "97b44773-2026-4bf4-bf79-711603fcf709", "metadata": {}, "outputs": [{"data": {"text/plain": "[(('2018-11', '90'),\n  (57144, 408476.3400000027, 67420.59999999996, '2018-11-30'))]"}, "execution_count": 93, "metadata": {}, "output_type": "execute_result"}], "source": "reduced_trips_rdd.take(1)"}, {"cell_type": "code", "execution_count": 89, "id": "7e67a6f4-62c6-4099-8a68-06157dc43408", "metadata": {}, "outputs": [], "source": "# Mapowanie ko\u0144cowe: (month, borough, passengers, total_amount, trip_distance, top_days)\nfinal_result_rdd = reduced_trips_rdd.map(lambda x: (x[0][0], x[0][1], x[1][0], x[1][1], x[1][2], [(x[1][3], x[1][0])]))"}, {"cell_type": "code", "execution_count": 92, "id": "d03402b7-bc48-453c-8e3c-e3fac42a9d15", "metadata": {}, "outputs": [{"data": {"text/plain": "[('2018-11',\n  '90',\n  57144,\n  408476.3400000027,\n  67420.59999999996,\n  [('2018-11-30', 57144)]),\n ('2018-11',\n  '125',\n  16160,\n  125007.52999999977,\n  22822.090000000007,\n  [('2018-11-30', 16160)])]"}, "execution_count": 92, "metadata": {}, "output_type": "execute_result"}], "source": "final_result_rdd.take(2)"}, {"cell_type": "code", "execution_count": 98, "id": "5d5a1d58-fc63-45b5-88f2-6705b93580bb", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Sortowanie wynik\u00f3w wed\u0142ug miesi\u0105ca, dzielnicy i liczby pasa\u017cer\u00f3w\nsorted_result_rdd = final_result_rdd.sortBy(lambda x: -x[2])"}, {"cell_type": "code", "execution_count": 99, "id": "3fbdc2af-ef19-475d-9d2a-725e02bb3082", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[('2018-12',\n  '230',\n  189531,\n  1507918.0399999723,\n  266780.25000000006,\n  [('2018-12-31', 189531)]),\n ('2018-12',\n  '237',\n  178880,\n  1155937.6599999866,\n  177964.53,\n  [('2018-12-31', 178880)]),\n ('2018-12',\n  '161',\n  175513,\n  1324813.05999998,\n  217574.11000000004,\n  [('2018-12-31', 175513)]),\n ('2018-11',\n  '230',\n  173294,\n  1383404.9399999774,\n  243677.45,\n  [('2018-11-30', 173294)]),\n ('2018-11',\n  '237',\n  172800,\n  1132096.1399999885,\n  171870.97999999995,\n  [('2018-11-30', 172800)]),\n ('2018-12',\n  '48',\n  161874,\n  1209878.0699999859,\n  218504.37999999995,\n  [('2018-12-31', 161874)]),\n ('2018-11',\n  '161',\n  161333,\n  1213314.3799999857,\n  194661.25999999998,\n  [('2018-11-30', 161333)]),\n ('2018-12',\n  '186',\n  161262,\n  1286360.9699999823,\n  207744.5,\n  [('2018-12-31', 161262)]),\n ('2018-12',\n  '236',\n  148932,\n  961168.2500000005,\n  155786.44,\n  [('2018-12-31', 148932)]),\n ('2018-11',\n  '236',\n  147122,\n  966804.2100000002,\n  153703.38000000003,\n  [('2018-11-30', 147122)])]"}, "execution_count": 99, "metadata": {}, "output_type": "execute_result"}], "source": "sorted_result_rdd.take(10)"}, {"cell_type": "code", "execution_count": 20, "id": "91d77fd7-1f15-4365-ae80-c902aeb55ce7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Zapis wyniku do pliku pickle\n# word_counts.saveAsPickleFile(rdd_result_dir)"}, {"cell_type": "markdown", "id": "42d8b5ec-b799-4177-8e4a-80a583d995e7", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 21, "id": "4325d378-b145-4e8f-8d37-80a072b506c3", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "28137d3d-6f0d-443f-97b8-38104aaced6d", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "6d045dae-5826-4015-8833-564d356db1f8", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "f7738406-c426-4238-b0fb-983f4585bc5a", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "5e7e569f-5f6b-4a98-b177-1b6fb0fc3333", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 22, "id": "6329c04b-3e50-41a8-93f1-333ac0ea64ce", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "4c2cfb0d-51b6-45bb-b173-ab8ac630d4f3", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** *misji g\u0142\u00f3wnej* swojego projektu oparte o *DataFrame API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *DataFrame API* nie jest w stanie wszystkiego \"naprawi\u0107\". \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 23, "id": "eca6e627-0ce5-4c48-b441-3bcc14e32f36", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import split, explode, count\n# Wczytanie danych\ndata = spark.read.text(datasource4_dir)"}, {"cell_type": "code", "execution_count": 24, "id": "bcc4aaa9-8dc2-4726-871e-5e2450ba3fa8", "metadata": {}, "outputs": [], "source": "# Dzielenie linii na s\u0142owa i eksplozja do osobnych wierszy\nwords = data.select(explode(split(data.value, \" \")).alias(\"word\"))\n\n# Zliczanie s\u0142\u00f3w\nword_counts = words.groupBy(\"word\").agg(count(\"word\").alias(\"count\"))"}, {"cell_type": "code", "execution_count": 25, "id": "45165cca-5197-4590-ba69-7541085147f9", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/01/10 21:07:36 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}], "source": "# Zapis wynik\u00f3w do tabeli \nword_counts.write.mode(\"overwrite\").saveAsTable(df_result_table)"}, {"cell_type": "markdown", "id": "d0797752-450e-4f8f-a1d4-93a890a62c3d", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 26, "id": "c3647eae-2801-46ac-b43d-74e5bbfcab52", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "3bed01aa-cc23-427e-84c8-e5b76b9323bb", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 3 - Pandas API on Spark\n\nTa cz\u0119\u015b\u0107 to wyzwanie. W szczeg\u00f3lno\u015bci dla os\u00f3b, kt\u00f3re nie programuj\u0105 na co dzie\u0144 w Pythonie, lub kt\u00f3re nie nie korzysta\u0142y do tej pory z Pandas API.  \n\nPowodzenia!\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "971a265f-db04-4a26-936d-18ab875ddffa", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "91621654-a24e-4ddb-b2c7-9f149252af13", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "9a5184ce-cf42-4342-aeec-b56c30b66bbd", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 27, "id": "63fd8306-87e9-46f2-b622-d60693e3ba6d", "metadata": {}, "outputs": [], "source": "#NIE ZMIENIA\u0106\nbefore_ps_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "a967f079-7106-4bd7-9d26-98ced2aeb43b", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** swojego projektu oparte o *Pandas API on Spark*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *Pandas API on Spark* nie jest w stanie wszystkiego \"naprawi\u0107\". \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 28, "id": "e2094a69-30b1-4970-825b-2b0624436cd5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n  warnings.warn(\n/usr/lib/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"}], "source": "import pyspark.pandas as ps\n\nlines_ps = ps.read_csv(datasource4_dir, header=None)"}, {"cell_type": "code", "execution_count": 29, "id": "ea69e909-a557-4294-b1ae-f0d551649eec", "metadata": {}, "outputs": [], "source": "words_ps = lines_ps[0].apply(lambda x: x.split(' ') if x is not None else []).explode().reset_index(drop=True)"}, {"cell_type": "code", "execution_count": 30, "id": "b5759f50-b92e-41a5-9eb0-9b00e2528ce5", "metadata": {}, "outputs": [], "source": "word_counts = words_ps.value_counts()"}, {"cell_type": "code", "execution_count": 31, "id": "b02917f4-e1f2-4fb4-8b53-8829fb3f0689", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas Series is expected to be small.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n                                                                                \r"}], "source": "word_counts_pandas = word_counts.head(50).to_pandas()"}, {"cell_type": "code", "execution_count": 32, "id": "76e0d7f7-82f3-41d4-8267-cf288f2f6e81", "metadata": {}, "outputs": [], "source": "word_counts_pandas.to_json(ps_result_file, orient='index')"}, {"cell_type": "markdown", "id": "298a0ec5-ab13-4e39-a572-e7adf8b8556a", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 33, "id": "108bee2a-a847-4625-8e4a-939951ac9201", "metadata": {}, "outputs": [], "source": "#NIE ZMIENIA\u0106\nafter_ps_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "e32e266b-b5cd-41d0-aeab-c1edc365910d", "metadata": {}, "source": "# Analiza wynik\u00f3w i wydajno\u015bci *misji g\u0142\u00f3wnych*"}, {"cell_type": "markdown", "id": "46b67111-62d0-4657-b158-1ed37db9ed96", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 1 - Spark Core (RDD)"}, {"cell_type": "code", "execution_count": 34, "id": "5cfc9900-7e0c-49ff-adba-e339f83ffe51", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "('\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"', 1)\n('1,\"EWR\",\"Newark', 1)\n('Zone\"', 260)\n('3,\"Bronx\",\"Allerton/Pelham', 1)\n('Heights\",\"Boro', 11)\n('7,\"Queens\",\"Astoria\",\"Boro', 1)\n('8,\"Queens\",\"Astoria', 1)\n('Park\",\"Boro', 22)\n('10,\"Queens\",\"Baisley', 1)\n('11,\"Brooklyn\",\"Bath', 1)\n('12,\"Manhattan\",\"Battery', 1)\n('Park\",\"Yellow', 3)\n('Ridge\",\"Boro', 1)\n('15,\"Queens\",\"Bay', 1)\n('Terrace/Fort', 1)\n('19,\"Queens\",\"Bellerose\",\"Boro', 1)\n('21,\"Brooklyn\",\"Bensonhurst', 1)\n('West\",\"Boro', 2)\n('23,\"Staten', 1)\n('Hill\",\"Boro', 11)\n('24,\"Manhattan\",\"Bloomingdale\",\"Yellow', 1)\n('Point/Fort', 1)\n('Hills\",\"Boro', 5)\n('29,\"Brooklyn\",\"Brighton', 1)\n('Channel\",\"Boro', 1)\n('31,\"Bronx\",\"Bronx', 1)\n('32,\"Bronx\",\"Bronxdale\",\"Boro', 1)\n('34,\"Brooklyn\",\"Brooklyn', 1)\n('35,\"Brooklyn\",\"Brownsville\",\"Boro', 1)\n('39,\"Brooklyn\",\"Canarsie\",\"Boro', 1)\n('40,\"Brooklyn\",\"Carroll', 1)\n('41,\"Manhattan\",\"Central', 1)\n('42,\"Manhattan\",\"Central', 1)\n('43,\"Manhattan\",\"Central', 1)\n('44,\"Staten', 1)\n('Island\",\"Charleston/Tottenville\",\"Boro', 1)\n('45,\"Manhattan\",\"Chinatown\",\"Yellow', 1)\n('Island\",\"Boro', 4)\n('47,\"Bronx\",\"Claremont/Bathgate\",\"Boro', 1)\n('48,\"Manhattan\",\"Clinton', 1)\n('50,\"Manhattan\",\"Clinton', 1)\n('West\",\"Yellow', 6)\n('51,\"Bronx\",\"Co-Op', 1)\n('52,\"Brooklyn\",\"Cobble', 1)\n('Point\",\"Boro', 4)\n('Street\",\"Boro', 1)\n('55,\"Brooklyn\",\"Coney', 1)\n('Club\",\"Boro', 1)\n('60,\"Bronx\",\"Crotona', 1)\n('61,\"Brooklyn\",\"Crown', 1)\n"}], "source": "# Wczytanie wynik\u00f3w z pliku pickle\nword_counts = sc.pickleFile(rdd_result_dir)\n\n# Wy\u015bwietlenie 50 pierwszych element\u00f3w\nresult_sample = word_counts.take(50)\nfor item in result_sample:\n    print(item)"}, {"cell_type": "code", "execution_count": 35, "id": "16edae69-8062-4422-842f-d50bca0af9a7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"numTasks\": 8,\n  \"numActiveTasks\": 0,\n  \"numCompleteTasks\": 6,\n  \"numFailedTasks\": 0,\n  \"numKilledTasks\": 0,\n  \"numCompletedIndices\": 6,\n  \"executorDeserializeTime\": 1658,\n  \"executorDeserializeCpuTime\": 1141389498,\n  \"executorRunTime\": 10246,\n  \"executorCpuTime\": 4764578987,\n  \"resultSize\": 10535,\n  \"jvmGcTime\": 170,\n  \"resultSerializationTime\": 2,\n  \"memoryBytesSpilled\": 0,\n  \"diskBytesSpilled\": 0,\n  \"peakExecutionMemory\": 0,\n  \"inputBytes\": 30805,\n  \"inputRecords\": 403,\n  \"outputBytes\": 12575,\n  \"outputRecords\": 5,\n  \"shuffleRemoteBlocksFetched\": 3,\n  \"shuffleLocalBlocksFetched\": 3,\n  \"shuffleFetchWaitTime\": 52,\n  \"shuffleRemoteBytesRead\": 4798,\n  \"shuffleRemoteBytesReadToDisk\": 0,\n  \"shuffleLocalBytesRead\": 5167,\n  \"shuffleReadBytes\": 9965,\n  \"shuffleReadRecords\": 30,\n  \"shuffleWriteBytes\": 6721,\n  \"shuffleWriteTime\": 29870117,\n  \"shuffleWriteRecords\": 20\n}\n"}], "source": "subtract_metrics(after_rdd_metrics, before_rdd_metrics)"}, {"cell_type": "markdown", "id": "efc730f1-4b5e-4a68-8a86-11768918fcf4", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)"}, {"cell_type": "code", "execution_count": 36, "id": "b950a09d-045e-4143-a3cf-8ecc7c73ac41", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-----+\n|                word|count|\n+--------------------+-----+\n|120,\"Manhattan\",\"...|    1|\n|123,\"Brooklyn\",\"H...|    1|\n|125,\"Manhattan\",\"...|    1|\n|182,\"Bronx\",\"Park...|    1|\n|   132,\"Queens\",\"JFK|    1|\n|  196,\"Queens\",\"Rego|    1|\n|171,\"Queens\",\"Murray|    1|\n|79,\"Manhattan\",\"East|    1|\n|Island\",\"Bloomfie...|    1|\n|        Hills\",\"Boro|    5|\n|Duyvil/Kingsbridg...|    1|\n|143,\"Manhattan\",\"...|    1|\n|114,\"Manhattan\",\"...|    1|\n|   134,\"Queens\",\"Kew|    1|\n|119,\"Bronx\",\"High...|    1|\n|28,\"Queens\",\"Bria...|    1|\n| Hill/Clifton\",\"Boro|    1|\n|Island\",\"Oakwood\"...|    1|\n|67,\"Brooklyn\",\"Dyker|    1|\n|      South\",\"Yellow|    6|\n|   135,\"Queens\",\"Kew|    1|\n|153,\"Manhattan\",\"...|    1|\n|      Island\",\"Great|    2|\n|         187,\"Staten|    1|\n|          Bay\",\"Boro|    4|\n|        Island\",\"New|    1|\n|  126,\"Bronx\",\"Hunts|    1|\n|147,\"Bronx\",\"Long...|    1|\n|         204,\"Staten|    1|\n|236,\"Manhattan\",\"...|    1|\n|37,\"Brooklyn\",\"Bu...|    1|\n|        Plaza\",\"Boro|    1|\n|174,\"Bronx\",\"Norw...|    1|\n|112,\"Brooklyn\",\"G...|    1|\n|        Field\",\"Boro|    1|\n|4,\"Manhattan\",\"Al...|    1|\n|         Club\",\"Boro|    1|\n|   247,\"Bronx\",\"West|    1|\n|38,\"Queens\",\"Cambria|    1|\n|164,\"Manhattan\",\"...|    1|\n|    15,\"Queens\",\"Bay|    1|\n|104,\"Manhattan\",\"...|    1|\n|        Slope\",\"Boro|    1|\n|232,\"Manhattan\",\"Two|    1|\n|82,\"Queens\",\"Elmh...|    1|\n|    86,\"Queens\",\"Far|    1|\n|167,\"Bronx\",\"Morr...|    1|\n|161,\"Manhattan\",\"...|    1|\n|      Terrace\",\"Boro|    1|\n|20,\"Bronx\",\"Belmo...|    1|\n+--------------------+-----+\nonly showing top 50 rows\n\n"}], "source": "df = spark.table(df_result_table)\n\n# Wy\u015bwietlenie 50 pierwszych rekord\u00f3w\ndf.show(50)"}, {"cell_type": "code", "execution_count": 37, "id": "3f344ed9-94c1-4d79-b839-1839548d8c67", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"numTasks\": 3,\n  \"numActiveTasks\": 0,\n  \"numCompleteTasks\": 2,\n  \"numFailedTasks\": 0,\n  \"numKilledTasks\": 0,\n  \"numCompletedIndices\": 2,\n  \"executorDeserializeTime\": 826,\n  \"executorDeserializeCpuTime\": 495080802,\n  \"executorRunTime\": 3364,\n  \"executorCpuTime\": 2105934130,\n  \"resultSize\": 7829,\n  \"jvmGcTime\": 146,\n  \"resultSerializationTime\": 3,\n  \"memoryBytesSpilled\": 0,\n  \"diskBytesSpilled\": 0,\n  \"peakExecutionMemory\": 67633136,\n  \"inputBytes\": 12322,\n  \"inputRecords\": 266,\n  \"outputBytes\": 6276,\n  \"outputRecords\": 423,\n  \"shuffleRemoteBlocksFetched\": 0,\n  \"shuffleLocalBlocksFetched\": 1,\n  \"shuffleFetchWaitTime\": 0,\n  \"shuffleRemoteBytesRead\": 0,\n  \"shuffleRemoteBytesReadToDisk\": 0,\n  \"shuffleLocalBytesRead\": 22770,\n  \"shuffleReadBytes\": 22770,\n  \"shuffleReadRecords\": 423,\n  \"shuffleWriteBytes\": 22770,\n  \"shuffleWriteTime\": 61718225,\n  \"shuffleWriteRecords\": 423\n}\n"}], "source": "subtract_metrics(after_df_metrics, before_df_metrics)"}, {"cell_type": "markdown", "id": "f063b46c-579d-4775-ba3f-837708279ea2", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 3 - Pandas API on Spark"}, {"cell_type": "code", "execution_count": 38, "id": "ab5e31a2-fd31-40ca-be7b-b20b13dc38a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"125\": 1,\n  \"7\": 1,\n  \"51\": 1,\n  \"124\": 1,\n  \"169\": 1,\n  \"205\": 1,\n  \"15\": 1,\n  \"54\": 1,\n  \"232\": 1,\n  \"234\": 1,\n  \"155\": 1,\n  \"132\": 1,\n  \"154\": 1,\n  \"200\": 1,\n  \"11\": 1,\n  \"101\": 1,\n  \"138\": 1,\n  \"29\": 1,\n  \"69\": 1,\n  \"42\": 1,\n  \"112\": 1,\n  \"73\": 1,\n  \"87\": 1,\n  \"64\": 1,\n  \"3\": 1,\n  \"30\": 1,\n  \"113\": 1,\n  \"34\": 1,\n  \"133\": 1,\n  \"59\": 1,\n  \"162\": 1,\n  \"139\": 1,\n  \"146\": 1,\n  \"250\": 1,\n  \"8\": 1,\n  \"160\": 1,\n  \"258\": 1,\n  \"22\": 1,\n  \"28\": 1,\n  \"184\": 1,\n  \"203\": 1,\n  \"199\": 1,\n  \"85\": 1,\n  \"16\": 1,\n  \"35\": 1,\n  \"52\": 1,\n  \"251\": 1,\n  \"171\": 1,\n  \"183\": 1,\n  \"187\": 1\n}\n"}], "source": "import json\n\n# Odczytaj zawarto\u015b\u0107 pliku JSON\nwith open(ps_result_file, 'r') as file:\n    json_content = json.load(file)\n\n# Wy\u015bwietl zawarto\u015b\u0107\nprint(json.dumps(json_content, indent=2))"}, {"cell_type": "code", "execution_count": 39, "id": "32788c91-3f8e-4fb1-8afc-5eb00938e687", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"numTasks\": 6,\n  \"numActiveTasks\": 0,\n  \"numCompleteTasks\": 5,\n  \"numFailedTasks\": 0,\n  \"numKilledTasks\": 0,\n  \"numCompletedIndices\": 5,\n  \"executorDeserializeTime\": 1127,\n  \"executorDeserializeCpuTime\": 700677879,\n  \"executorRunTime\": 10020,\n  \"executorCpuTime\": 2968441424,\n  \"resultSize\": 20219,\n  \"jvmGcTime\": 122,\n  \"resultSerializationTime\": 8,\n  \"memoryBytesSpilled\": 0,\n  \"diskBytesSpilled\": 0,\n  \"peakExecutionMemory\": 67633136,\n  \"inputBytes\": 49288,\n  \"inputRecords\": 799,\n  \"outputBytes\": 0,\n  \"outputRecords\": 0,\n  \"shuffleRemoteBlocksFetched\": 0,\n  \"shuffleLocalBlocksFetched\": 1,\n  \"shuffleFetchWaitTime\": 0,\n  \"shuffleRemoteBytesRead\": 0,\n  \"shuffleRemoteBytesReadToDisk\": 0,\n  \"shuffleLocalBytesRead\": 11978,\n  \"shuffleReadBytes\": 11978,\n  \"shuffleReadRecords\": 266,\n  \"shuffleWriteBytes\": 11978,\n  \"shuffleWriteTime\": 31967786,\n  \"shuffleWriteRecords\": 266\n}\n"}], "source": "subtract_metrics(after_ps_metrics, before_ps_metrics)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}