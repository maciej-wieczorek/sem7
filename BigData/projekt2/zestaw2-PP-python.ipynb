{"cells":[{"cell_type":"markdown","id":"861b6d4a-0de6-42ba-97a5-beef1f82f292","metadata":{},"source":["# Projekt Apache Spark"]},{"cell_type":"markdown","id":"7b301ae8-ceff-4dbf-8d04-75bb4eb52480","metadata":{},"source":["# Wprowadzenie\n","\n","Wykorzystując ten notatnik jako szablon zrealizuj projekt Apache Spark zgodnie z przydzielonym zestawem. \n","\n","Kilka uwag:\n","\n","* Nie modyfikuj ani nie usuwaj paragrafów *markdown* w tym notatniku, chyba że wynika to jednoznacznie z instrukcji. \n","* Istniejące paragrafy zawierające *kod* uzupełnij w razie potrzeby zgodnie z instrukcjami\n","    - nie usuwaj ich\n","    - nie usuwaj zawartych w nich instrukcji oraz kodu\n","    - nie modyfikuj ich, jeśli instrukcje jawnie tego nie nakazują\n","* Możesz dodawać nowe paragrafy zarówno zawierające kod jak i komentarze dotyczące tego kodu (markdown)"]},{"cell_type":"markdown","id":"e69d12f1-1013-4c74-b6aa-686ccfcbdd5c","metadata":{},"source":["# Treść projektu\n","\n","Poniżej w paragrafie markdown wstaw tytuł przydzielonego zestawu"]},{"cell_type":"markdown","id":"adfc4ff6-4d43-49ed-a0d1-8b6988eaec16","metadata":{},"source":["# Zestaw 2 – nyc-taxi\n","\n","**Uwaga**\n","\n","- W ramach wzorca nie są spełnione żadne reguły projektu. \n","- Brak konsekwencji w wykorzystaniu właściwego API w ramach poszczególnych części\n","- Zadanie *misji głównej* polega na zliczeniu słówek.  "]},{"cell_type":"markdown","id":"5e128e43-6cce-4ffa-9609-9fae4b164ae9","metadata":{},"source":["# Działania wstępne \n","\n","Uruchom poniższy paragraf, aby utworzyć obiekty kontekstu Sparka. Jeśli jest taka potrzeba dostosuj te polecenia. Pamiętaj po potrzebnych bibliotekach."]},{"cell_type":"code","execution_count":1,"id":"26fb1050-386f-4398-ba5a-b45f5065d87b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/01/14 19:44:45 INFO SparkEnv: Registering MapOutputTracker\n","24/01/14 19:44:45 INFO SparkEnv: Registering BlockManagerMaster\n","24/01/14 19:44:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/01/14 19:44:45 INFO SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Spark session & context\n","spark = SparkSession.builder.getOrCreate()\n","\n","sc = spark.sparkContext"]},{"cell_type":"markdown","id":"8695a354-52bc-4bba-8222-7121bf07ae90","metadata":{},"source":["W poniższym paragrafie uzupełnij polecenia definiujące poszczególne zmienne. \n","\n","Pamiętaj abyś:\n","\n","* w późniejszym kodzie, dla wszystkich cześci projektu, korzystał z tych zdefiniowanych zmiennych. Wykorzystuj je analogicznie jak parametry\n","* przed ostateczną rejestracją projektu usunął ich wartości, tak aby nie pozostawiać w notatniku niczego co mogłoby identyfikować Ciebie jako jego autora"]},{"cell_type":"code","execution_count":2,"id":"e883af01-7117-4faa-a840-7ff807a195d9","metadata":{},"outputs":[],"source":["# pełna ścieżka do katalogu w zasobniku zawierającego podkatalogi `datasource1` i `datasource4` \n","# z danymi źródłowymi\n","input_dir = \"/home/maciejmail_wieczorek/projekt1/input\""]},{"cell_type":"markdown","id":"4601cc7a-3ed5-47e2-994f-ebec642049b5","metadata":{},"source":["Nie modyfikuj poniższych paragrafów. Wykonaj je i używaj zdefniowanych poniżej zmiennych jak parametrów Twojego programu."]},{"cell_type":"code","execution_count":3,"id":"6167e297-01ed-463e-bb81-9104d7cf7093","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","# ścieżki dla danych źródłowych \n","datasource1_dir = input_dir + \"/datasource1\"\n","datasource4_dir = input_dir + \"/datasource4\"\n","\n","# nazwy i ścieżki dla wyników dla misji głównej \n","# część 1 (Spark Core - RDD) \n","rdd_result_dir = \"/tmp/output1\"\n","\n","# część 2 (Spark SQL - DataFrame)\n","df_result_table = \"output2\"\n","\n","# część 3 (Pandas API on Spark)\n","ps_result_file = \"/tmp/output3.json\""]},{"cell_type":"code","execution_count":4,"id":"e36e0314-a4ac-4096-9e4b-23fd4a73e0a9","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","import os\n","def remove_file(file):\n","    if os.path.exists(file):\n","        os.remove(file)\n","\n","remove_file(\"metric_functions.py\")\n","remove_file(\"tools_functions.py\")"]},{"cell_type":"code","execution_count":5,"id":"1b4b8e00-10ae-47dc-b623-d1dacbe9c86b","metadata":{},"outputs":[{"data":{"text/plain":["3322"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# NIE ZMIENIAĆ\n","import requests\n","r = requests.get(\"https://jankiewicz.pl/bigdata/metric_functions.py\", allow_redirects=True)\n","open('metric_functions.py', 'wb').write(r.content)\n","r = requests.get(\"https://jankiewicz.pl/bigdata/tools_functions.py\", allow_redirects=True)\n","open('tools_functions.py', 'wb').write(r.content)"]},{"cell_type":"code","execution_count":6,"id":"0a433894-dc97-46f2-be51-9f40fa36894f","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","%run metric_functions.py\n","%run tools_functions.py"]},{"cell_type":"markdown","id":"c9d3a9dc-ac3b-4316-abb9-365caa1d7185","metadata":{},"source":["Poniższe paragrafy mają na celu usunąć ewentualne pozostałości poprzednich uruchomień tego lub innych notatników"]},{"cell_type":"code","execution_count":7,"id":"08091c72-937f-41c2-9afe-d1505862bf1c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["rm: `/tmp/output1': No such file or directory\n"]},{"name":"stdout","output_type":"stream","text":["Error deleting file /tmp/output1: Command '['hadoop', 'fs', '-rm', '-r', '/tmp/output1']' returned non-zero exit status 1.\n"]}],"source":["# NIE ZMIENIAĆ\n","# usunięcie miejsca docelowego dla część 1 (Spark Core - RDD) \n","delete_dir(spark, rdd_result_dir)"]},{"cell_type":"code","execution_count":8,"id":"f3e863c0-c824-47bd-b53a-ce3b1fd6d453","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"]},{"name":"stdout","output_type":"stream","text":["The table output2 does not exist.\n","Error deleting file file:/spark-warehouse/output2: Command '['hadoop', 'fs', '-rm', '-r', 'file:/spark-warehouse/output2']' returned non-zero exit status 1.\n"]},{"name":"stderr","output_type":"stream","text":["rm: `file:/spark-warehouse/output2': No such file or directory\n"]}],"source":["# NIE ZMIENIAĆ\n","# usunięcie miejsca docelowego dla część 2 (Spark SQL - DataFrame) \n","drop_table(spark, df_result_table)"]},{"cell_type":"code","execution_count":9,"id":"72956a1a-da48-4d2b-a07a-e03d56431d6e","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","# usunięcie miejsca docelowego dla część 3 (Pandas API on Spark) \n","remove_file(ps_result_file)"]},{"cell_type":"code","execution_count":10,"id":"b9e423d4-92b8-4161-98da-1a867f86d780","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://pbd-cluster-m.europe-central2-c.c.big-data-2023-10-mw.internal:44557\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7ff6d75a7820>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# NIE ZMIENIAĆ\n","spark"]},{"cell_type":"markdown","id":"14faf05b-6c52-4b02-b2e5-2ddb3f38c704","metadata":{},"source":["***Uwaga!***\n","\n","Uruchom poniższy paragraf i sprawdź czy adres, pod którym dostępny *Apache Spark Application UI* jest poprawny wywołując następny testowy paragraf. \n","\n","W razie potrzeby określ samodzielnie poprawny adres, pod którym dostępny *Apache Spark Application UI*"]},{"cell_type":"code","execution_count":11,"id":"32acf3d2-ec4e-469d-bb0b-5f260c2c8e3b","metadata":{},"outputs":[{"data":{"text/plain":["'http://pbd-cluster-m.europe-central2-c.c.big-data-2023-10-mw.internal:44557'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# adres URL, pod którym dostępny Apache Spark Application UI (REST API)\n","# \n","spark_ui_address = extract_host_and_port(spark, \"http://localhost:4041\")\n","spark_ui_address"]},{"cell_type":"code","execution_count":12,"id":"32c2329e-1d7a-465f-a23b-333f95bf7deb","metadata":{},"outputs":[{"data":{"text/plain":["{'numTasks': 0,\n"," 'numActiveTasks': 0,\n"," 'numCompleteTasks': 0,\n"," 'numFailedTasks': 0,\n"," 'numKilledTasks': 0,\n"," 'numCompletedIndices': 0,\n"," 'executorDeserializeTime': 0,\n"," 'executorDeserializeCpuTime': 0,\n"," 'executorRunTime': 0,\n"," 'executorCpuTime': 0,\n"," 'resultSize': 0,\n"," 'jvmGcTime': 0,\n"," 'resultSerializationTime': 0,\n"," 'memoryBytesSpilled': 0,\n"," 'diskBytesSpilled': 0,\n"," 'peakExecutionMemory': 0,\n"," 'inputBytes': 0,\n"," 'inputRecords': 0,\n"," 'outputBytes': 0,\n"," 'outputRecords': 0,\n"," 'shuffleRemoteBlocksFetched': 0,\n"," 'shuffleLocalBlocksFetched': 0,\n"," 'shuffleFetchWaitTime': 0,\n"," 'shuffleRemoteBytesRead': 0,\n"," 'shuffleRemoteBytesReadToDisk': 0,\n"," 'shuffleLocalBytesRead': 0,\n"," 'shuffleReadBytes': 0,\n"," 'shuffleReadRecords': 0,\n"," 'shuffleWriteBytes': 0,\n"," 'shuffleWriteTime': 0,\n"," 'shuffleWriteRecords': 0}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# testowy paragraf\n","test_metrics = get_current_metrics(spark_ui_address)\n","test_metrics"]},{"cell_type":"markdown","id":"f5ccca69-c577-440c-aa5c-c9df3a54e127","metadata":{},"source":["# Część 1 - Spark Core (RDD)\n","\n","## Misje poboczne\n","\n","W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "]},{"cell_type":"code","execution_count":null,"id":"f0af3440-983a-4cac-a8e7-4908b010947c","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5fc37879-e0fa-4c4a-bd0d-4c01c3ecf38a","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"d303a72b-4083-470e-b25d-3224360ee94f","metadata":{},"source":["## Misja główna \n","\n","Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n","\n","Nie musisz go uruchamiać podczas implementacji rozwiązania."]},{"cell_type":"code","execution_count":13,"id":"037689d7-f0ee-4165-bef0-83fa7f3e8346","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","before_rdd_metrics = get_current_metrics(spark_ui_address)"]},{"cell_type":"markdown","id":"b23971c0-cec7-4ea8-befb-7f063dce863c","metadata":{},"source":["W poniższych paragrafach wprowadź **rozwiązanie** *misji głównej* oparte na *RDD API*. \n","\n","Pamiętaj o wydajności Twojego przetwarzania, *RDD API* tego wymaga. \n","\n","Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."]},{"cell_type":"code","execution_count":14,"id":"8af00c41-02a9-4a85-b3c6-bc41098edbe2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Wczytanie plików tekstowych\n","trips_rdd = sc.textFile(datasource1_dir).map(lambda x: x.split(',')).map(lambda x: (x[1][:7], x[1][8:10], int(x[3]), float(x[4]), x[7], x[9], float(x[16])))\n","MONTH = 0\n","DAY = 1\n","PASSENGER_COUNT = 2\n","TRIP_DISTANCE = 3\n","PULOCATIONID = 4\n","PAYMENT_TYPE = 5\n","TOTAL_AMOUNT = 6\n","\n","taxi_zones_rdd = sc.textFile(datasource4_dir)\n","taxi_zones_header = taxi_zones_rdd.first()\n","taxi_zones_rdd = taxi_zones_rdd.filter(lambda x: x != taxi_zones_header).map(lambda x: x.split(',')).map(lambda x: (x[0], x[1].replace('\"', '')))"]},{"cell_type":"code","execution_count":15,"id":"a02125db-8f02-427a-9aa1-5cbdf2fec36f","metadata":{},"outputs":[],"source":["trips_filtered_rdd = trips_rdd.filter(lambda x: x[PAYMENT_TYPE] == '2')"]},{"cell_type":"code","execution_count":16,"id":"f1292ff0-49c6-4294-85b9-3888a4db6ab5","metadata":{},"outputs":[],"source":["merged_rdd = trips_filtered_rdd.keyBy(lambda x: x[PULOCATIONID]).join(taxi_zones_rdd.keyBy(lambda x: x[0])) \\\n","    .map(lambda x: (x[1][0][0], x[1][0][1], x[1][0][2], x[1][0][3], x[1][0][4], x[1][0][5], x[1][0][6], x[1][1][1]))"]},{"cell_type":"code","execution_count":17,"id":"b6977477-7036-49fa-b4fe-1f3a942c719f","metadata":{},"outputs":[],"source":["# agregacja (month, borough) -> (sum: passenger_count, sum: total_amount, sum: trip_distance, count_trips)\n","agg_boroughs = merged_rdd.map(lambda x: ((x[MONTH],x[-1]),(x[PASSENGER_COUNT], x[TRIP_DISTANCE], x[TOTAL_AMOUNT], 1))).reduceByKey(lambda x,y: (x[0]+y[0],\\\n","    x[1]+y[1], x[2]+y[2], x[3]+y[3]))"]},{"cell_type":"code","execution_count":18,"id":"d2f68804-dcb2-49a6-b6f5-fabc5c9442e9","metadata":{},"outputs":[],"source":["filtered_agg_boroughs = agg_boroughs.filter(lambda x: x[1][-1] >= 1000)"]},{"cell_type":"code","execution_count":19,"id":"cc39cd2e-7c5e-4261-b694-f12cf9b56a1d","metadata":{},"outputs":[],"source":["from heapq import nlargest\n","top_boroughs = filtered_agg_boroughs.groupBy(lambda x: x[0][0]).flatMap(lambda x: nlargest(3, x[1], key=lambda y: y[-1]))"]},{"cell_type":"code","execution_count":20,"id":"1249d5f7-9e9c-4e04-b272-2081fc3abc21","metadata":{},"outputs":[],"source":["# agregacja (month, borough, day) -> (count_trips)\n","agg_days = merged_rdd.map(lambda x: ((x[MONTH], x[-1], x[DAY]), 1)).reduceByKey(lambda x,y: x+y)"]},{"cell_type":"code","execution_count":21,"id":"1c668af7-bdc6-4011-8a1b-5fdd42809c0c","metadata":{},"outputs":[],"source":["top_days = agg_days.groupBy(lambda x: (x[0][0], x[0][1])).flatMap(lambda x: nlargest(3, x[1], key=lambda y: y[1])).map(lambda x: (x[0][0], x[0][1], x[0][2], x[1]))"]},{"cell_type":"code","execution_count":22,"id":"2db99978-eb20-4e91-bfc7-8a38a561e0db","metadata":{},"outputs":[],"source":["top_days_collected = top_days.groupBy(lambda x: (x[0], x[1])).map(lambda x: (x[0],[(y[2], y[3]) for y in x[1]]))"]},{"cell_type":"code","execution_count":23,"id":"f43abb4e-f980-49f5-975f-97853326e274","metadata":{},"outputs":[],"source":["final_rdd = top_boroughs.join(top_days_collected)"]},{"cell_type":"code","execution_count":24,"id":"b7f92b41-11cb-440c-93f5-318f22c2c61d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[(('2018-11', 'Queens'),\n","  ((251199, 1501460.9200000179, 5314633.170000963, 154931),\n","   [('16', 6653), ('27', 5813), ('01', 5718)])),\n"," (('2018-12', 'Manhattan'),\n","  ((3748256, 4843509.109999997, 27917296.41002932, 2280603),\n","   [('01', 87599), ('08', 85825), ('15', 84043)])),\n"," (('2018-12', 'Queens'),\n","  ((264815, 1525387.2300000037, 5389044.16000096, 159229),\n","   [('27', 6310), ('21', 6108), ('28', 5987)])),\n"," (('2018-11', 'Unknown'),\n","  ((64540, 105209.79999999962, 625450.179999888, 44413),\n","   [('10', 1923), ('23', 1859), ('14', 1710)])),\n"," (('2018-12', 'Unknown'),\n","  ((75779, 118279.03999999938, 697061.9300001608, 50576),\n","   [('08', 2279), ('19', 2030), ('18', 1966)])),\n"," (('2018-11', 'Manhattan'),\n","  ((3459143, 4469622.089999998, 26047331.7500267, 2142171),\n","   [('10', 87158), ('03', 85184), ('17', 84468)]))]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["final_rdd.take(10)"]},{"cell_type":"code","execution_count":25,"id":"91d77fd7-1f15-4365-ae80-c902aeb55ce7","metadata":{},"outputs":[],"source":["# Zapis wyniku do pliku pickle\n","# word_counts.saveAsPickleFile(rdd_result_dir)"]},{"cell_type":"markdown","id":"42d8b5ec-b799-4177-8e4a-80a583d995e7","metadata":{},"source":["Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n","\n","Nie musisz go uruchamiać podczas implementacji rozwiązania."]},{"cell_type":"code","execution_count":26,"id":"4325d378-b145-4e8f-8d37-80a072b506c3","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","after_rdd_metrics = get_current_metrics(spark_ui_address)"]},{"cell_type":"markdown","id":"28137d3d-6f0d-443f-97b8-38104aaced6d","metadata":{},"source":["# Część 2 - Spark SQL (DataFrame)\n","\n","## Misje poboczne\n","\n","W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "]},{"cell_type":"code","execution_count":null,"id":"6d045dae-5826-4015-8833-564d356db1f8","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f7738406-c426-4238-b0fb-983f4585bc5a","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"5e7e569f-5f6b-4a98-b177-1b6fb0fc3333","metadata":{},"source":["## Misja główna \n","\n","Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n","\n","Nie musisz go uruchamiać podczas implementacji rozwiązania."]},{"cell_type":"code","execution_count":27,"id":"6329c04b-3e50-41a8-93f1-333ac0ea64ce","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","before_df_metrics = get_current_metrics(spark_ui_address)"]},{"cell_type":"markdown","id":"4c2cfb0d-51b6-45bb-b173-ab8ac630d4f3","metadata":{},"source":["W poniższych paragrafach wprowadź **rozwiązanie** *misji głównej* swojego projektu oparte o *DataFrame API*. \n","\n","Pamiętaj o wydajności Twojego przetwarzania, *DataFrame API* nie jest w stanie wszystkiego \"naprawić\". \n","\n","Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."]},{"cell_type":"code","execution_count":28,"id":"eca6e627-0ce5-4c48-b441-3bcc14e32f36","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import split, explode, count, col\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import rank, sum, collect_list, struct, row_number\n","# Wczytanie danych\n","trips_data = spark.read.text(datasource1_dir)\n","taxi_zones_data = spark.read.text(datasource4_dir)\n","\n","trips_df = trips_data.select(\n","    split(trips_data['value'], ',').alias('data')\n",").select(\n","    col('data')[1].substr(0, 7).alias('month'),\n","    col('data')[1].substr(9, 2).alias('day'),\n","    col('data')[3].cast('int').alias('Passenger_count'),\n","    col('data')[4].cast('float').alias('Trip_distance'),\n","    col('data')[7].alias('PULocationID'),\n","    col('data')[9].alias('Payment_type'),\n","    col('data')[16].cast('float').alias('Total_amount')\n",")\n","\n","taxi_zones_df = taxi_zones_data.select(split(taxi_zones_data['value'], ',').alias('data')\n",").select(\n","    col('data')[0].alias('LocationID'),\n","    col('data')[1].alias('Borough'),\n",")\n","\n","taxi_zones_df = taxi_zones_df.exceptAll(taxi_zones_df.limit(1))"]},{"cell_type":"code","execution_count":29,"id":"64e6e1d5-5d73-4550-8fe6-3a1b986c28a9","metadata":{},"outputs":[],"source":["trips_filtered_df = trips_df.filter(col(\"Payment_type\") == '2')"]},{"cell_type":"code","execution_count":30,"id":"37185059-7472-4093-a56c-890ace5c717b","metadata":{},"outputs":[],"source":["merged_df = trips_filtered_df.join(taxi_zones_df, col(\"PULocationID\") == col(\"LocationID\"))"]},{"cell_type":"code","execution_count":31,"id":"5f853224-8627-47bf-aaf9-c2373dda847b","metadata":{},"outputs":[],"source":["agg_boroughs = merged_df.groupBy(\"month\", \"Borough\").agg(\n","    sum(\"Passenger_count\").alias(\"passengers\"),\n","    sum(\"Total_amount\").alias(\"total_amount\"),\n","    sum(\"Trip_distance\").alias(\"trip_distance\"),\n","    count(\"*\").alias(\"count_trips\")\n",").orderBy(\"month\", \"Borough\")\n","\n","filtered_agg_boroughs = agg_boroughs.filter(col(\"count_trips\") >= 1000)"]},{"cell_type":"code","execution_count":32,"id":"190a9022-3c4c-4b88-a6b1-4244ef9efcf2","metadata":{},"outputs":[],"source":["windowSpec = Window.partitionBy(\"month\").orderBy(col(\"passengers\").desc())\n","ranked_boroughs = filtered_agg_boroughs.withColumn(\"rank\", rank().over(windowSpec))\n","top_boroughs = ranked_boroughs.filter(col(\"rank\") <= 3).orderBy(col(\"month\").desc())"]},{"cell_type":"code","execution_count":33,"id":"64d4b6d3-e3d4-470a-9f5d-e89e7e6797ef","metadata":{},"outputs":[],"source":["agg_days = merged_df.groupBy(\"month\", \"Borough\", \"day\").agg(\n","    count(\"*\").alias(\"count_trips\")\n",").orderBy(\"month\", \"Borough\", col(\"count_trips\").desc())"]},{"cell_type":"code","execution_count":34,"id":"7d3a8df0-9926-4210-8ab2-6093a0f3a162","metadata":{},"outputs":[],"source":["windowSpec = Window.partitionBy(\"month\", \"Borough\").orderBy(col(\"count_trips\").desc())\n","ranked_data = agg_days.withColumn(\"rank\", rank().over(windowSpec))\n","top_days = ranked_data.filter(col(\"rank\") <= 3).orderBy(col(\"month\").desc(), \"Borough\", col(\"count_trips\").desc())"]},{"cell_type":"code","execution_count":35,"id":"01428719-9193-466d-bba6-74cc89fca796","metadata":{},"outputs":[],"source":["top_days_collected = top_days.groupBy(\"month\", \"Borough\").agg(collect_list(struct(\"day\", \"count_trips\")).alias(\"top_days\"))"]},{"cell_type":"code","execution_count":36,"id":"a6243748-6607-449d-bd4b-8f3c7c03734e","metadata":{},"outputs":[],"source":["final_df = top_boroughs.join(top_days_collected, on=[\"month\", \"Borough\"], how=\"left\")"]},{"cell_type":"code","execution_count":37,"id":"44fe68f0-4cac-45a2-8438-744146f0f31b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+-----------+----------+--------------------+------------------+---------------------------------------+\n","|month  |Borough    |passengers|total_amount        |trip_distance     |top_days                               |\n","+-------+-----------+----------+--------------------+------------------+---------------------------------------+\n","|2018-11|\"Queens\"   |251199    |5314633.200125992   |1501460.9201596621|[{16, 6653}, {27, 5813}, {01, 5718}]   |\n","|2018-11|\"Unknown\"  |64540     |625450.1823387146   |105209.80002824217|[{10, 1923}, {23, 1859}, {14, 1710}]   |\n","|2018-12|\"Queens\"   |264815    |5389044.19103837    |1525387.2301812489|[{27, 6310}, {21, 6108}, {28, 5987}]   |\n","|2018-12|\"Manhattan\"|3748256   |2.7917296492852896E7|4843509.109785054 |[{01, 87599}, {08, 85825}, {15, 84043}]|\n","|2018-11|\"Manhattan\"|3459143   |2.604733185194066E7 |4469622.089800246 |[{10, 87158}, {03, 85184}, {17, 84468}]|\n","|2018-12|\"Unknown\"  |75779     |697061.9320377801   |118279.04001246765|[{08, 2279}, {19, 2030}, {18, 1966}]   |\n","+-------+-----------+----------+--------------------+------------------+---------------------------------------+\n","\n"]}],"source":["final_df.select(\"month\", \"Borough\", \"passengers\", \"total_amount\", \"trip_distance\", \"top_days\").show(truncate=False)"]},{"cell_type":"markdown","id":"d0797752-450e-4f8f-a1d4-93a890a62c3d","metadata":{},"source":["Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n","\n","Nie musisz go uruchamiać podczas implementacji rozwiązania."]},{"cell_type":"code","execution_count":38,"id":"c3647eae-2801-46ac-b43d-74e5bbfcab52","metadata":{},"outputs":[],"source":["# NIE ZMIENIAĆ\n","after_df_metrics = get_current_metrics(spark_ui_address)"]},{"cell_type":"markdown","id":"3bed01aa-cc23-427e-84c8-e5b76b9323bb","metadata":{},"source":["# Część 3 - Pandas API on Spark\n","\n","Ta część to wyzwanie. W szczególności dla osób, które nie programują na co dzień w Pythonie, lub które nie nie korzystały do tej pory z Pandas API.  \n","\n","Powodzenia!\n","\n","## Misje poboczne\n","\n","W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "]},{"cell_type":"code","execution_count":null,"id":"971a265f-db04-4a26-936d-18ab875ddffa","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"91621654-a24e-4ddb-b2c7-9f149252af13","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"9a5184ce-cf42-4342-aeec-b56c30b66bbd","metadata":{},"source":["## Misja główna \n","\n","Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n","\n","Nie musisz go uruchamiać podczas implementacji rozwiązania."]},{"cell_type":"code","execution_count":39,"id":"63fd8306-87e9-46f2-b622-d60693e3ba6d","metadata":{},"outputs":[],"source":["#NIE ZMIENIAĆ\n","before_ps_metrics = get_current_metrics(spark_ui_address)"]},{"cell_type":"markdown","id":"a967f079-7106-4bd7-9d26-98ced2aeb43b","metadata":{},"source":["W poniższych paragrafach wprowadź **rozwiązanie** swojego projektu oparte o *Pandas API on Spark*. \n","\n","Pamiętaj o wydajności Twojego przetwarzania, *Pandas API on Spark* nie jest w stanie wszystkiego \"naprawić\". \n","\n","Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."]},{"cell_type":"code","execution_count":40,"id":"e2094a69-30b1-4970-825b-2b0624436cd5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n","  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"]}],"source":["import pyspark.pandas as ps\n","\n","trips_ps = ps.read_csv(datasource1_dir, header=None)\n","trips_ps.columns = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'Passenger_count', 'Trip_distance', 'RateCodeID', 'Store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'Payment_type', 'Fare_amount', 'Extra', 'MTA_tax', 'Tip_amount', 'Tolls_amount', 'Improvement_surcharge', 'Total_amount']\n","trips_ps['month'] = trips_ps['tpep_pickup_datetime'].dt.strftime('%Y-%m')\n","trips_ps['day'] = trips_ps['tpep_pickup_datetime'].dt.day\n","trips_ps = trips_ps.drop(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'RateCodeID', 'Store_and_fwd_flag', 'DOLocationID', 'Fare_amount', 'Extra', 'MTA_tax', 'Tip_amount', 'Tolls_amount', 'Improvement_surcharge'], axis=1)\n","taxi_zones_ps = ps.read_csv(datasource4_dir)\n","taxi_zones_ps = taxi_zones_ps.drop(['Zone', 'service_zone'], axis=1)"]},{"cell_type":"code","execution_count":41,"id":"f468efb6-f2c0-4d6f-9b0c-c6c6801ddd6e","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LocationID</th>\n","      <th>Borough</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>EWR</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Queens</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Bronx</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Manhattan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Staten Island</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   LocationID        Borough\n","0           1            EWR\n","1           2         Queens\n","2           3          Bronx\n","3           4      Manhattan\n","4           5  Staten Island"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["taxi_zones_ps.head()"]},{"cell_type":"code","execution_count":42,"id":"95ec6845-89d0-4760-ab6c-473e60e5aa2c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Passenger_count</th>\n","      <th>Trip_distance</th>\n","      <th>PULocationID</th>\n","      <th>Payment_type</th>\n","      <th>Total_amount</th>\n","      <th>month</th>\n","      <th>day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2.06</td>\n","      <td>142</td>\n","      <td>1</td>\n","      <td>14.04</td>\n","      <td>2018-12</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1.10</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>8.15</td>\n","      <td>2018-11</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2.13</td>\n","      <td>114</td>\n","      <td>1</td>\n","      <td>17.88</td>\n","      <td>2018-11</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2.40</td>\n","      <td>249</td>\n","      <td>1</td>\n","      <td>15.95</td>\n","      <td>2018-12</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1.42</td>\n","      <td>143</td>\n","      <td>1</td>\n","      <td>12.36</td>\n","      <td>2018-11</td>\n","      <td>29</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Passenger_count  Trip_distance  PULocationID  Payment_type  Total_amount    month  day\n","0                1           2.06           142             1         14.04  2018-12   15\n","1                1           1.10            79             1          8.15  2018-11   25\n","2                2           2.13           114             1         17.88  2018-11   28\n","3                1           2.40           249             1         15.95  2018-12    7\n","4                1           1.42           143             1         12.36  2018-11   29"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["trips_ps.head()"]},{"cell_type":"code","execution_count":43,"id":"06fafdce-b24c-4da1-9ee7-38b2cbec6826","metadata":{},"outputs":[],"source":["trips_filtered_ps = trips_ps[trips_ps[\"Payment_type\"] == '2']"]},{"cell_type":"code","execution_count":44,"id":"c2d7ade5-a068-437c-b592-445da4fc8cb9","metadata":{},"outputs":[],"source":["merged_ps = trips_filtered_ps.merge(taxi_zones_ps, left_on=\"PULocationID\", right_on=\"LocationID\")"]},{"cell_type":"code","execution_count":45,"id":"4c33d454-e13a-4b00-a63d-f1abdb6bc532","metadata":{},"outputs":[],"source":["agg_boroughs = merged_ps.groupby([\"month\", \"Borough\"]).agg(\n","    passengers=(\"Passenger_count\", \"sum\"),\n","    total_amount=(\"Total_amount\", \"sum\"),\n","    trip_distance=(\"Trip_distance\", \"sum\"),\n","    count_trips=(\"month\", \"count\")\n",").reset_index()"]},{"cell_type":"code","execution_count":46,"id":"2ebad05b-9bc4-43e0-b9ef-1e74b7029053","metadata":{},"outputs":[],"source":["filtered_agg_boroughs = agg_boroughs[agg_boroughs[\"count_trips\"] >= 1000]"]},{"cell_type":"code","execution_count":47,"id":"a430d144-8b66-4a8b-ad64-4077976565d7","metadata":{},"outputs":[],"source":["top_boroughs = filtered_agg_boroughs.sort_values(by=['month', 'count_trips'], ascending=[True, False]).groupby('month').head(3).reset_index()"]},{"cell_type":"code","execution_count":48,"id":"fa884752-2aad-4b20-a039-94013ad26141","metadata":{},"outputs":[],"source":["agg_days = merged_ps.groupby([\"month\", \"Borough\", \"day\"]).agg(\n","    count_trips=(\"month\", \"count\")\n",").reset_index()"]},{"cell_type":"code","execution_count":49,"id":"aad0e67b-cf89-4d87-9235-030326fc2f88","metadata":{},"outputs":[],"source":["top_days = agg_days.sort_values(by=[\"month\", \"Borough\", \"count_trips\"], ascending=[True, True, False]).groupby([\"month\", \"Borough\"]).head(3).reset_index()"]},{"cell_type":"code","execution_count":50,"id":"e585ddce-e812-4a1a-b322-1681ab444785","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `grouby.apply`, it is expensive to infer the data type internally.\n","  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n","                                                                                \r"]}],"source":["top_days_collected = top_days.groupby([\"month\", \"Borough\"])[['day', 'count_trips']].apply(lambda x: x.values.tolist()).reset_index(name='top_days')"]},{"cell_type":"code","execution_count":51,"id":"916d59e0-3831-4920-a110-2f7d0384af05","metadata":{},"outputs":[],"source":["final_ps = top_boroughs.merge(top_days_collected, on=['month', 'Borough'])"]},{"cell_type":"code","execution_count":52,"id":"c1abe2af-b96d-4694-841a-ab6bf99cc38e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>month</th>\n","      <th>Borough</th>\n","      <th>passengers</th>\n","      <th>total_amount</th>\n","      <th>trip_distance</th>\n","      <th>count_trips</th>\n","      <th>top_days</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7</td>\n","      <td>2018-12</td>\n","      <td>Unknown</td>\n","      <td>75779</td>\n","      <td>6.970619e+05</td>\n","      <td>118279.04</td>\n","      <td>50576</td>\n","      <td>[[8, 2279], [19, 2030], [18, 1966]]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17</td>\n","      <td>2018-12</td>\n","      <td>Queens</td>\n","      <td>264815</td>\n","      <td>5.389044e+06</td>\n","      <td>1525387.23</td>\n","      <td>159229</td>\n","      <td>[[27, 6310], [21, 6108], [28, 5987]]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2018-11</td>\n","      <td>Unknown</td>\n","      <td>64540</td>\n","      <td>6.254502e+05</td>\n","      <td>105209.80</td>\n","      <td>44413</td>\n","      <td>[[10, 1923], [23, 1859], [14, 1710]]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>2018-12</td>\n","      <td>Manhattan</td>\n","      <td>3748256</td>\n","      <td>2.791730e+07</td>\n","      <td>4843509.11</td>\n","      <td>2280603</td>\n","      <td>[[1, 87599], [8, 85825], [15, 84043]]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10</td>\n","      <td>2018-11</td>\n","      <td>Manhattan</td>\n","      <td>3459143</td>\n","      <td>2.604733e+07</td>\n","      <td>4469622.09</td>\n","      <td>2142171</td>\n","      <td>[[10, 87158], [3, 85184], [17, 84468]]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>12</td>\n","      <td>2018-11</td>\n","      <td>Queens</td>\n","      <td>251199</td>\n","      <td>5.314633e+06</td>\n","      <td>1501460.92</td>\n","      <td>154931</td>\n","      <td>[[16, 6653], [27, 5813], [1, 5718]]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index    month    Borough  passengers  total_amount  trip_distance  count_trips                                top_days\n","0      7  2018-12    Unknown       75779  6.970619e+05      118279.04        50576     [[8, 2279], [19, 2030], [18, 1966]]\n","1     17  2018-12     Queens      264815  5.389044e+06     1525387.23       159229    [[27, 6310], [21, 6108], [28, 5987]]\n","2      0  2018-11    Unknown       64540  6.254502e+05      105209.80        44413    [[10, 1923], [23, 1859], [14, 1710]]\n","3      2  2018-12  Manhattan     3748256  2.791730e+07     4843509.11      2280603   [[1, 87599], [8, 85825], [15, 84043]]\n","4     10  2018-11  Manhattan     3459143  2.604733e+07     4469622.09      2142171  [[10, 87158], [3, 85184], [17, 84468]]\n","5     12  2018-11     Queens      251199  5.314633e+06     1501460.92       154931     [[16, 6653], [27, 5813], [1, 5718]]"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["final_ps.head(10)"]},{"cell_type":"markdown","id":"298a0ec5-ab13-4e39-a572-e7adf8b8556a","metadata":{},"source":["Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n","\n","Nie musisz go uruchamiać podczas implementacji rozwiązania."]},{"cell_type":"code","execution_count":53,"id":"108bee2a-a847-4625-8e4a-939951ac9201","metadata":{},"outputs":[],"source":["#NIE ZMIENIAĆ\n","after_ps_metrics = get_current_metrics(spark_ui_address)"]},{"cell_type":"markdown","id":"e32e266b-b5cd-41d0-aeab-c1edc365910d","metadata":{},"source":["# Analiza wyników i wydajności *misji głównych*"]},{"cell_type":"markdown","id":"46b67111-62d0-4657-b158-1ed37db9ed96","metadata":{},"source":["## Część 1 - Spark Core (RDD)"]},{"cell_type":"code","execution_count":54,"id":"5cfc9900-7e0c-49ff-adba-e339f83ffe51","metadata":{},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o1643.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /tmp/output1\n\tat org.apache.hadoop.mapred.LocatedFileStatusFetcher.getFileStatuses(LocatedFileStatusFetcher.java:198)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:252)\n\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: /tmp/output1\n\tat org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallable.call(LocatedFileStatusFetcher.java:402)\n\tat org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallable.call(LocatedFileStatusFetcher.java:380)\n\tat org.apache.hadoop.thirdparty.com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)\n\tat org.apache.hadoop.thirdparty.com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:69)\n\tat org.apache.hadoop.thirdparty.com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_61491/537720179.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Wyświetlenie 50 pierwszych elementów\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \"\"\"\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[T]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"RDD[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/miniconda3/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1643.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /tmp/output1\n\tat org.apache.hadoop.mapred.LocatedFileStatusFetcher.getFileStatuses(LocatedFileStatusFetcher.java:198)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:252)\n\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: /tmp/output1\n\tat org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallable.call(LocatedFileStatusFetcher.java:402)\n\tat org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallable.call(LocatedFileStatusFetcher.java:380)\n\tat org.apache.hadoop.thirdparty.com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)\n\tat org.apache.hadoop.thirdparty.com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:69)\n\tat org.apache.hadoop.thirdparty.com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"]}],"source":["# Wczytanie wyników z pliku pickle\n","word_counts = sc.pickleFile(rdd_result_dir)\n","\n","# Wyświetlenie 50 pierwszych elementów\n","result_sample = word_counts.take(50)\n","for item in result_sample:\n","    print(item)"]},{"cell_type":"code","execution_count":null,"id":"16edae69-8062-4422-842f-d50bca0af9a7","metadata":{},"outputs":[],"source":["subtract_metrics(after_rdd_metrics, before_rdd_metrics)"]},{"cell_type":"markdown","id":"efc730f1-4b5e-4a68-8a86-11768918fcf4","metadata":{},"source":["## Część 2 - Spark SQL (DataFrame)"]},{"cell_type":"code","execution_count":null,"id":"b950a09d-045e-4143-a3cf-8ecc7c73ac41","metadata":{},"outputs":[],"source":["df = spark.table(df_result_table)\n","\n","# Wyświetlenie 50 pierwszych rekordów\n","df.show(50)"]},{"cell_type":"code","execution_count":null,"id":"3f344ed9-94c1-4d79-b839-1839548d8c67","metadata":{},"outputs":[],"source":["subtract_metrics(after_df_metrics, before_df_metrics)"]},{"cell_type":"markdown","id":"f063b46c-579d-4775-ba3f-837708279ea2","metadata":{},"source":["## Część 3 - Pandas API on Spark"]},{"cell_type":"code","execution_count":null,"id":"ab5e31a2-fd31-40ca-be7b-b20b13dc38a2","metadata":{},"outputs":[],"source":["import json\n","\n","# Odczytaj zawartość pliku JSON\n","with open(ps_result_file, 'r') as file:\n","    json_content = json.load(file)\n","\n","# Wyświetl zawartość\n","print(json.dumps(json_content, indent=2))"]},{"cell_type":"code","execution_count":null,"id":"32788c91-3f8e-4fb1-8afc-5eb00938e687","metadata":{},"outputs":[],"source":["subtract_metrics(after_ps_metrics, before_ps_metrics)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
